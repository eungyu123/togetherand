import { useEffect, useRef, RefObject } from 'react';
import { useCallStore } from '@/domain/call/store/call';

export const useAudioDetection = ({
  audioElement,
  isCurrentUser,
  elementRef,
}: {
  audioElement: HTMLAudioElement | null;
  isCurrentUser?: boolean;
  elementRef: RefObject<HTMLDivElement | null>;
}) => {
  // íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ì™€ í™€ë“œ íƒ€ì„ì„ ìœ„í•œ ref
  const lastSpeakingTime = useRef<number>(0);
  const currentState = useRef<'silent' | 'speaking'>('silent');

  useEffect(() => {
    const stream = audioElement?.srcObject as MediaStream;

    if (!stream) {
      return;
    }

    const detectAudioStream = () => {
      // MediaStreamì—ì„œ ì˜¤ë””ì˜¤ íŠ¸ë™ ì¶”ì¶œ
      const audioTrack = stream.getAudioTracks()[0];

      // ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
      if (!audioTrack) return;

      // Web Audio API ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë¸Œë¼ìš°ì € í˜¸í™˜ì„± ê³ ë ¤)
      const audioCtx = new (window.AudioContext || (window as any).webkitAudioContext)();
      // ì£¼íŒŒìˆ˜ ë¶„ì„ì„ ìœ„í•œ ë¶„ì„ê¸° ìƒì„±
      const analyser = audioCtx.createAnalyser();
      // ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ì„ ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ì— ì—°ê²°
      const source = audioCtx.createMediaStreamSource(stream);

      // ë¶„ì„ê¸°ì— ì†ŒìŠ¤ ì—°ê²°
      source.connect(analyser);
      // FFT í¬ê¸° ì„¤ì • (ì£¼íŒŒìˆ˜ ë¶„ì„ì˜ ì •ë°€ë„)
      analyser.fftSize = 256;
      const bufferLength = analyser.frequencyBinCount;
      // ì£¼íŒŒìˆ˜ ë°ì´í„°ë¥¼ ì €ì¥í•  ë°°ì—´
      const dataArray = new Uint8Array(bufferLength);

      /**
       * ë³¼ë¥¨ ê°’ì„ dBë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
       */
      const volumeToDb = (volume: number): number => {
        if (volume === 0) return -Infinity;
        return 20 * Math.log10(volume / 255);
      };

      /**
       * ì‹¤ì‹œê°„ìœ¼ë¡œ ë³¼ë¥¨ì„ ì²´í¬í•˜ëŠ” í•¨ìˆ˜ (íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ì™€ í™€ë“œ íƒ€ì„ ì ìš©)
       */
      const checkVolume = () => {
        // ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        analyser.getByteFrequencyData(dataArray);
        // í‰ê·  ë³¼ë¥¨ ê³„ì‚°
        const average = dataArray.reduce((a, b) => a + b) / bufferLength;

        // ë³¼ë¥¨ì„ dBë¡œ ë³€í™˜
        const volumeDb = volumeToDb(average);

        // íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì„ê³„ê°’ ì„¤ì • (ë” ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ì¡°ì •)
        // -35dB: ìƒë‹¹íˆ ë“¤ë¦¬ëŠ” ì†Œë¦¬ (ì¼ë°˜ì ì¸ ëŒ€í™” ìˆ˜ì¤€)
        // -40dB: ì¡°ìš©í•œ ëŒ€í™” ìˆ˜ì¤€
        // -50dB: ë§¤ìš° ì¡°ìš©í•œ ì†Œë¦¬ (ê±°ì˜ ë“¤ë¦¬ì§€ ì•ŠìŒ)
        const turnOnThreshold = -35; // ë§í•˜ê¸° ì‹œì‘ ì„ê³„ê°’ (ë” ë†’ê²Œ)
        const turnOffThreshold = -40; // ë§í•˜ê¸° ì¢…ë£Œ ì„ê³„ê°’ (ë” ë†’ê²Œ)

        const currentTime = Date.now();
        const holdTime = 300; // 0.3ì´ˆ í™€ë“œ íƒ€ì„ (ì´ê²Œ ì œì¼ ìì—°ìŠ¤ëŸ¬ìš´ë“¯ ì˜¤ëŠ˜ì€ 9ì›”8ì¼)

        let newState = currentState.current;

        if (currentState.current === 'silent') {
          // ë¬´ìŒ ìƒíƒœì—ì„œ ë§í•˜ê¸° ì‹œì‘ ì¡°ê±´
          if (volumeDb >= turnOnThreshold) {
            newState = 'speaking';
            lastSpeakingTime.current = currentTime;
          }
        } else {
          // ë§í•˜ê¸° ìƒíƒœì—ì„œ ë¬´ìŒìœ¼ë¡œ ì „í™˜ ì¡°ê±´
          if (volumeDb < turnOffThreshold) {
            // í™€ë“œ íƒ€ì„ ì²´í¬
            if (currentTime - lastSpeakingTime.current >= holdTime) {
              newState = 'silent';
            }
          } else {
            // ì—¬ì „íˆ ë§í•˜ê³  ìˆìœ¼ë©´ ì‹œê°„ ì—…ë°ì´íŠ¸
            lastSpeakingTime.current = currentTime;
          }
        }

        // ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì—…ë°ì´íŠ¸
        if (newState !== currentState.current) {
          currentState.current = newState;

          // DOM ìš”ì†Œì— ì§ì ‘ CSS í´ë˜ìŠ¤ì™€ ìŠ¤íƒ€ì¼ í† ê¸€ (ë¦¬ë Œë”ë§ ì—†ì´)
          if (elementRef?.current) {
            const element = elementRef.current;
            element.classList.toggle('speaking', newState === 'speaking');

            if (newState === 'speaking') {
              const borderColor = isCurrentUser ? 'white' : '#3b82f6';
              const shadowColor = isCurrentUser
                ? '0 0 20px rgba(255,255,255,0.3)'
                : '0 0 20px rgba(59,130,246,0.3)';
              element.style.setProperty('--speaking-border', borderColor);
              element.style.setProperty('--speaking-shadow', shadowColor);
            }
          }

          // ë””ë²„ê¹…ìš© ë¡œê·¸ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
          // console.log(`ìŒì„± ê°ì§€ ìƒíƒœ ë³€ê²½: ${newState}, ë³¼ë¥¨: ${volumeDb.toFixed(2)}dB`);
        }

        // ë‹¤ìŒ í”„ë ˆì„ì—ì„œ ë‹¤ì‹œ ì²´í¬ (60fps)
        requestAnimationFrame(checkVolume);
      };

      // ë³¼ë¥¨ ì²´í¬ ì‹œì‘
      checkVolume();
    };

    // í˜„ì¬ ì‚¬ìš©ìì´ê³  ë¡œì»¬ ìŠ¤íŠ¸ë¦¼ì´ ìˆìœ¼ë©´ ë°”ë¡œ ê°ì§€ ì‹œì‘
    if (isCurrentUser && audioElement) {
      detectAudioStream();
    }

    if (!isCurrentUser && audioElement) {
      // ì˜¤ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ê°€ ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆìœ¼ë©´ ë°”ë¡œ ê°ì§€ ì‹œì‘
      if (audioElement.readyState >= 2) {
        detectAudioStream();
      } else {
        // console.log('ğŸ” ì˜¤ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ê°€ ì¤€ë¹„ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì¤€ë¹„ë˜ë©´ ê°ì§€ ì‹œì‘');
        audioElement.addEventListener('canplay', detectAudioStream);
      }
    }

    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì œê±°
    return () => {
      if (audioElement) {
        audioElement.removeEventListener('canplay', detectAudioStream);
      }
    };
  }, [audioElement, isCurrentUser]);
};
